{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTqdl2PXh7-n"
      },
      "source": [
        "IMPORTING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h83Ovj7OpVeY"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import urllib\n",
        "import bs4 as bs\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import random\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from wikipedia import page\n",
        "import random\n",
        "import string\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "%matplotlib inline\n",
        "from pandas import DataFrame\n",
        "import pyttsx3\n",
        "import speech_recognition as sr\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('popular', quiet=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvPVjm_Nj2f-"
      },
      "source": [
        "INSTALLING LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "34h6Mr95U8pO"
      },
      "outputs": [],
      "source": [
        "!pip install speechRecognition\n",
        "!pip install PyAudio\n",
        "!pip install wikipedia\n",
        "!pip install bs4\n",
        "!pip install gtts\n",
        "!pip install spacy\n",
        "!pip install pyspellchecker\n",
        "!pip install pyttsx3\n",
        "!pip install urllib\n",
        "!pip install python-engineio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2sGzkZrpeVt"
      },
      "outputs": [],
      "source": [
        "page1=requests.get('https://www.timeanddate.com/weather/india')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4leFTzzMpsQ0"
      },
      "outputs": [],
      "source": [
        "def temp(topic):\n",
        "\n",
        "    page = page1\n",
        "    soup = BeautifulSoup(page.content,'html.parser')\n",
        "\n",
        "    data = soup.find(class_ = 'zebra fw tb-wt zebra va-m')\n",
        "\n",
        "    tags = data('a')\n",
        "    city = [tag.contents[0] for tag in tags]\n",
        "    tags2 = data.find_all(class_ = 'rbi')\n",
        "    temp = [tag.contents[0] for tag in tags2]\n",
        "\n",
        "    indian_weather = pd.DataFrame(\n",
        "    {\n",
        "        'City':city,\n",
        "        'Temperature':temp\n",
        "    }\n",
        "    )\n",
        "\n",
        "    df = indian_weather[indian_weather['City'].str.contains(topic.title())]\n",
        "\n",
        "    return (df['Temperature'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvIiKo0mp4LC"
      },
      "outputs": [],
      "source": [
        "import bs4 as bs\n",
        "def wiki_data(topic):\n",
        "\n",
        "    topic=topic.title()\n",
        "    topic=topic.replace(' ', '_',1)\n",
        "    url1=\"https://en.wikipedia.org/wiki/\"\n",
        "    url=url1+topic\n",
        "\n",
        "    source = urllib.request.urlopen(url).read()\n",
        "\n",
        "    # Parsing the data/ creating BeautifulSoup object\n",
        "    soup = bs.BeautifulSoup(source,'lxml')\n",
        "\n",
        "    # Fetching the data\n",
        "    text = \"\"\n",
        "    for paragraph in soup.find_all('p'):\n",
        "        text += paragraph.text\n",
        "\n",
        "    import re\n",
        "    # Preprocessing the data\n",
        "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
        "    text = re.sub(r'\\s+',' ',text)\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d',' ',text)\n",
        "    text = re.sub(r'\\s+',' ',text)\n",
        "\n",
        "\n",
        "    return (text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI_C9YyLqHds"
      },
      "outputs": [],
      "source": [
        "def rem_special(text):\n",
        "    remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "    return(text.translate(remove_punct_dict))\n",
        "\n",
        "sample_text=\"I am sorry! I don't understand you.\"\n",
        "rem_special(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9exLEQfHqOsp"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def stemmer(text):\n",
        "    words = word_tokenize(text)\n",
        "    for w in words:\n",
        "        text=text.replace(w,PorterStemmer().stem(w))\n",
        "    return text\n",
        "\n",
        "stemmer(\"He is Eating. He played yesterday. He will be going tomorrow.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8L_DSR6kqWpf"
      },
      "outputs": [],
      "source": [
        "lemmer = WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "sample_text=\"rocks corpora better\" #default noun\n",
        "LemTokens(nltk.word_tokenize(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-2xJBj2qc3I"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "tokenizer = ToktokTokenizer()\n",
        "\n",
        "stopword_list = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "remove_stopwords(\"This is a sample sentence, showing off the stop words filtration.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvgoANBXqkvS"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "spacy_df=[]\n",
        "spacy_df1=[]\n",
        "df_spacy_nltk=pd.DataFrame()\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process whole documents\n",
        "sample_text = (\"The heavens are above. The moral code of conduct is above the civil code of conduct\")\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Token and Tag\n",
        "for token in doc:\n",
        "    spacy_df.append(token.pos_)\n",
        "    spacy_df1.append(token)\n",
        "\n",
        "\n",
        "df_spacy_nltk['origional']=spacy_df1\n",
        "df_spacy_nltk['spacy']=spacy_df\n",
        "#df_spacy_nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfGcV9g0q1XG"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "spacy_df=[]\n",
        "spacy_df1=[]\n",
        "df_spacy_nltk=pd.DataFrame()\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Process whole documents\n",
        "sample_text = (\"The heavens are above. The moral code of conduct is above the civil code of conduct\")\n",
        "doc = nlp(sample_text)\n",
        "\n",
        "# Token and Tag\n",
        "for token in doc:\n",
        "    spacy_df.append(token.pos_)\n",
        "    spacy_df1.append(token)\n",
        "\n",
        "\n",
        "df_spacy_nltk['origional']=spacy_df1\n",
        "df_spacy_nltk['spacy']=spacy_df\n",
        "#df_spacy_nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFswZTdVq-Iz"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def senti(text):\n",
        "    testimonial = TextBlob(text)\n",
        "    return(testimonial.polarity)\n",
        "\n",
        "sample_text=\"This apple is good\"\n",
        "print(\"polarity\",senti(sample_text))\n",
        "sample_text=\"This apple is not good\"\n",
        "print(\"polarity\",senti(sample_text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_860UQzrJ7j"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "\n",
        "def spelling(text):\n",
        "    splits = sample_text.split()\n",
        "    for split in splits:\n",
        "        text=text.replace(split,spell.correction(split))\n",
        "\n",
        "    return (text)\n",
        "\n",
        "\n",
        "sample_text=\"hapenning elephnt texte luckno sweeto\"\n",
        "spelling(sample_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaQvvcPWtl0X"
      },
      "outputs": [],
      "source": [
        "#TOkenisation\n",
        "print(nltk.sent_tokenize(\"Hey how are you? I am fine.\"))\n",
        "print(nltk.word_tokenize(\"Hey how are you? I am fine.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tnqWX-LhtzKh"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "documentA = 'This is about Messi'\n",
        "documentB = 'This is about TFIDF'\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors = vectorizer.fit_transform([documentA, documentB])\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "dense = vectors.todense()\n",
        "denselist = dense.tolist()\n",
        "df = pd.DataFrame(denselist, columns=feature_names)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vU-tP0o8uBNW"
      },
      "outputs": [],
      "source": [
        "def speak(message):\n",
        "    import pyttsx3\n",
        "    import engineio\n",
        "    gTTS(text = message,lang = \"en\",slow = False)\n",
        "    gTTS(text = \"hello\",lang = \"en\",slow = False)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgYA7wn9vt-G"
      },
      "outputs": [],
      "source": [
        "city = {}\n",
        "city[\"lucknow\"] = [\"lucknow\", \"lko\"]\n",
        "city[\"delhi\"]=[\"new delhi\",\"ndls\"]\n",
        "city[\"mumbai\"]=[\"mumbai\",\"bombay\"]\n",
        "city[\"kolkata\"]=[\"kolkata\",\"Calcutta\"]\n",
        "city[\"goa\"]=[\"goa\",\"ga\"]\n",
        "city[\"bengaluru\"]=[\"bengaluru\",\"bangalore\"]\n",
        "city[\"jaipur\"]=[\"jaipur\",\"jai\"]\n",
        "city[\"shimla\"]=[\"shimla\",\"shimla\"]\n",
        "#city[\"kolkata\"]=[\"kolkata\",\"Calcutta\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7B6BTj7gEKNv"
      },
      "outputs": [],
      "source": [
        "def city_name(sentence):\n",
        "    for word in sentence.split():\n",
        "        for key, values in city.items():\n",
        "\n",
        "            if word.lower() in values:\n",
        "                return(key)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MOZ5IoKERRR"
      },
      "outputs": [],
      "source": [
        "def LemNormalize(text):\n",
        "    text=rem_special(text) #remove special char\n",
        "    text=text.lower() # lower case\n",
        "    text=remove_stopwords(text) # remove stop words\n",
        "\n",
        "    return LemTokens(nltk.word_tokenize(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUhlfYmnEYDI"
      },
      "outputs": [],
      "source": [
        "#Generating answer\n",
        "def response(user_input):\n",
        "\n",
        "    ToGu_response=''\n",
        "    sent_tokens.append(user_input)\n",
        "\n",
        "\n",
        "\n",
        "    word_vectorizer = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    all_word_vectors = word_vectorizer.fit_transform(sent_tokens)\n",
        "\n",
        "\n",
        "    similar_vector_values = cosine_similarity(all_word_vectors[-1], all_word_vectors)\n",
        "    idx=similar_vector_values.argsort()[0][-2]\n",
        "\n",
        "\n",
        "    matched_vector = similar_vector_values.flatten()\n",
        "    matched_vector.sort()\n",
        "    vector_matched = matched_vector[-2]\n",
        "\n",
        "    if(vector_matched==0):\n",
        "        ToGu_response=ToGu_response+\"I am sorry! I don't understand you.\"\n",
        "        return ToGu_response\n",
        "    else:\n",
        "        ToGu_response = ToGu_response+sent_tokens[idx]\n",
        "        return ToGu_response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZksC--kEg86"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "topic=str(input(\"Please enter the city name you want to ask queries for: \"))\n",
        "topic=city_name(topic)\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "\n",
        "text=wiki_data(topic)\n",
        "\n",
        "sent_tokens = nltk.sent_tokenize(text)# converts to list of sentences\n",
        "word_tokens = nltk.word_tokenize(text)# converts to list of words\n",
        "weather_reading=(temp(topic)).iloc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M6fOGLvIMHO"
      },
      "outputs": [],
      "source": [
        "# greetings Keyword matching\n",
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "\n",
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnRKEXxAIshc"
      },
      "outputs": [],
      "source": [
        "PLACES_INPUTS = (\"places\", \"monuments\", \"buildings\",\"places\", \"monument\", \"building\")\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def ner(sentence):\n",
        "    places_imp=\"\"\n",
        "    doc = nlp(sentence)\n",
        "    for ent in doc.ents:\n",
        "        if (ent.label_==\"FAC\"):\n",
        "            #print(ent.text, ent.label_)\n",
        "            places_imp=places_imp+ent.text+\",\"+\" \"\n",
        "\n",
        "    return(places_imp)\n",
        "\n",
        "\n",
        "places_imp=ner(text)\n",
        "\n",
        "\n",
        "s=places_imp\n",
        "l = s.split()\n",
        "k = []\n",
        "for i in l:\n",
        "\n",
        "    # If condition is used to store unique string\n",
        "    # in another list 'k'\n",
        "    if (s.count(i)>1 and (i not in k)or s.count(i)==1):\n",
        "      k.append(i)\n",
        "\n",
        "PLACES_RESPONSES = ' '.join(k)\n",
        "\n",
        "def places(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in PLACES_INPUTS:\n",
        "            return (PLACES_RESPONSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdnaUzhKI60O"
      },
      "outputs": [],
      "source": [
        "WEATHER_INPUTS = (\"weather\", \"temp\", \"temperature\")\n",
        "\n",
        "WEATHER_RESPONSES =weather_reading\n",
        "\n",
        "def weather(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in WEATHER_INPUTS:\n",
        "            return (WEATHER_RESPONSES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FoI-5RkBJBEe",
        "outputId": "afb1b430-fc1f-4381-f096-b3d236f9f33b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ToGu: Hello\n",
            "Sentiment score= 0.0\n",
            "ToGu: hi there\n",
            "Sentiment score= 0.0\n",
            "ToGu: 68 °F\n"
          ]
        }
      ],
      "source": [
        "import engineio\n",
        "from gtts import gTTS\n",
        "#import espeak\n",
        "continue_dialogue=True\n",
        "print(\"ToGu: Hello\")\n",
        "speak(\"Hello\")\n",
        "\n",
        "while(continue_dialogue==True):\n",
        "    user_input = input(\"User:\")\n",
        "    user_input=user_input.lower()\n",
        "    user_input=spelling(user_input) #spelling check\n",
        "    print(\"Sentiment score=\",senti(user_input)) #sentiment score\n",
        "\n",
        "    if(user_input!='bye'):\n",
        "        if(user_input=='thanks' or user_input=='thank you' ):\n",
        "            print(\"ToGu: You are welcome..\")\n",
        "            speak(\" You are welcome\")\n",
        "\n",
        "        else:\n",
        "            if(greeting(user_input)!=None):\n",
        "                tmp=greeting(user_input)\n",
        "                print(\"ToGu: \"+tmp)\n",
        "                speak(tmp)\n",
        "\n",
        "            elif(weather(user_input)!=None):\n",
        "                tmp=weather(user_input)\n",
        "                print(\"ToGu: \"+tmp)\n",
        "                speak(tmp)\n",
        "\n",
        "\n",
        "            elif(places(user_input)!=None):\n",
        "                tmp=places(user_input)\n",
        "                print(\"ToGu: Important places are \"+tmp)\n",
        "                speak(\"Important places are\")\n",
        "                speak(tmp)\n",
        "            else:\n",
        "                print(\"ToGu: \",end=\"\")\n",
        "                temp=response(user_input)\n",
        "                print(temp)\n",
        "                speak(temp)\n",
        "                sent_tokens.remove(user_input)\n",
        "\n",
        "\n",
        "    else:\n",
        "        continue_dialogue=False\n",
        "        print(\"ToGu: Goodbye.\")\n",
        "        speak(\"goodbye\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n-R7mYnmiTh"
      },
      "outputs": [],
      "source": [
        "!pip install tk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwfGkKBL6zcJ"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "from base64 import b64decode\n",
        "from google.colab import output\n",
        "from IPython.display import Javascript\n",
        "\n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "def record(sec=3):\n",
        "  print(\"Speak Now...\")\n",
        "  display(Javascript(RECORD))\n",
        "  sec += 1\n",
        "  s = output.eval_js('record(%d)' % (sec*1000))\n",
        "  print(\"Done Recording !\")\n",
        "  b = b64decode(s.split(',')[1])\n",
        "  return b #byte stream\n",
        "record()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}